{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c69fff-ab3b-49c8-b85b-95fef1250249",
   "metadata": {},
   "source": [
    "<img src=\"../images/globus-logo.png\" width=250 alt=\"Globus logo\"></img>\n",
    "<img src=\"../images/esgf.png\" width=250 alt=\"ESGF logo\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483dcdb6-e125-4a52-a21f-55cfe1000dea",
   "metadata": {},
   "source": [
    "# ENSO Calculations using Globus Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a415308-0e9a-470c-bb68-da75b349c006",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this workflow, we combine topics covered in previous Pythia Foundations and CMIP6 Cookbook content to compute the [Niño 3.4 Index](https://climatedataguide.ucar.edu/climate-data/nino-sst-indices-nino-12-3-34-4-oni-and-tni) to multiple datasets, with the primary computations occuring on a remote machine. As a refresher of what the ENSO 3.4 index is, please see the following text, which is also included in the [ENSO Xarray](https://foundations.projectpythia.org/core/xarray/enso-xarray.html) content in the Pythia Foundations content.\n",
    "\n",
    "> Niño 3.4 (5N-5S, 170W-120W): The Niño 3.4 anomalies may be thought of as representing the average equatorial SSTs across the Pacific from about the dateline to the South American coast. The Niño 3.4 index typically uses a 5-month running mean, and El Niño or La Niña events are defined when the Niño 3.4 SSTs exceed +/- 0.4C for a period of six months or more.\n",
    "\n",
    "> Niño X Index computation: a) Compute area averaged total SST from Niño X region; b) Compute monthly climatology (e.g., 1950-1979) for area averaged total SST from Niño X region, and subtract climatology from area averaged total SST time series to obtain anomalies; c) Smooth the anomalies with a 5-month running mean; d) Normalize the smoothed values by its standard deviation over the climatological period.\n",
    "\n",
    "![](https://www.ncdc.noaa.gov/monitoring-content/teleconnections/nino-regions.gif)\n",
    "\n",
    "The previous cookbook, we ran this in a single notebook locally. In this example, we aim to execute the workflow on a remote machine, with only the visualizion of the dataset occuring locally.\n",
    "\n",
    "The overall goal of this tutorial is to introduce the idea of functions as a service with Globus, and how this can be used to calculate ENSO indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c6aed-a9c5-4d29-bfa3-c8e8be230567",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Intro to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Necessary | |\n",
    "| [hvPlot Basics](https://hvplot.holoviz.org/getting_started/hvplot.html) | Necessary | Interactive Visualization with hvPlot |\n",
    "| [Understanding of NetCDF](https://foundations.projectpythia.org/core/data-formats/netcdf-cf.html) | Helpful | Familiarity with metadata structure |\n",
    "| [Calculating ENSO with Xarray](https://foundations.projectpythia.org/core/xarray/enso-xarray.html) | Neccessary | Understanding of Masking and Xarray Functions |\n",
    "| Dask | Helpful | |\n",
    "\n",
    "- **Time to learn**: 30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff38f37-8f14-443f-b0c7-188baf75d1be",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bcfa1a-3907-446d-b384-29e97b5c8cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import hvplot.xarray\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from intake_esgf import ESGFCatalog\n",
    "import xarray as xr\n",
    "import cf_xarray\n",
    "import warnings\n",
    "import os\n",
    "from globus_compute_sdk import Executor, Client\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252748e9-c3a4-4018-8b9b-c26c40465faf",
   "metadata": {},
   "source": [
    "## Accessing our Data and Computing the ENSO 3.4 Index\n",
    "As mentioned in the introduction, we are utilizing functions from the previous ENSO notebooks. In order to run these with Globus Compute, we need to comply with the following requirements\n",
    "- All libraries/packages used in the function need to be installed on the globus compute endpoint\n",
    "- All functions/libraries/packages need to be imported and defined within the function to execute\n",
    "- The output from the function needs to serializable (ex. xarray.Dataset, numpy.array)\n",
    "\n",
    "Using these constraints, we setup the following function, with the key parameter being which modeling center (model) to compare. Two examples here include The National Center for Atmospheric Research (NCAR) and the Model for Interdisciplinary Research on Climate (MIROC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74d939-f87d-4a44-9e4a-6643b7d04fe7",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def run_plot_enso(model, return_path=False):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from intake_esgf import ESGFCatalog\n",
    "    import xarray as xr\n",
    "    import cf_xarray\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    def search_esgf(institution_id, grid='gn'):\n",
    "    \n",
    "        # Search and load the ocean surface temperature (tos)\n",
    "        cat = ESGFCatalog()\n",
    "        cat.search(\n",
    "            activity_id=\"CMIP\",\n",
    "            experiment_id=\"historical\",\n",
    "            institution_id=institution_id,\n",
    "            variable_id=[\"tos\"],\n",
    "            member_id='r11i1p1f1',\n",
    "            table_id=\"Omon\",\n",
    "        )\n",
    "\n",
    "        # Load into a datatree to represent different grids\n",
    "        dt = cat.to_datatree()\n",
    "\n",
    "        # If there are more than 1 grid, subset the one of interest\n",
    "        if len(dt.children) >= 2:\n",
    "            ds = dt[grid].to_dataset()\n",
    "        else:\n",
    "            ds = dt[\"tos\"].to_dataset()\n",
    "        return ds\n",
    "\n",
    "    def calculate_enso(ds):\n",
    "\n",
    "        # Subset the El Nino 3.4 index region\n",
    "        dso = ds.where(\n",
    "        (ds.cf[\"latitude\"] < 5) & (ds.cf[\"latitude\"] > -5) & (ds.cf[\"longitude\"] > 190) & (ds.cf[\"longitude\"] < 240), drop=True\n",
    "        )\n",
    "\n",
    "        # Calculate the monthly means\n",
    "        gb = dso.tos.groupby('time.month')\n",
    "\n",
    "        # Subtract the monthly averages, returning the anomalies\n",
    "        tos_nino34_anom = gb - gb.mean(dim='time')\n",
    "\n",
    "        # Determine the non-time dimensions and average using these\n",
    "        non_time_dims = set(tos_nino34_anom.dims)\n",
    "        non_time_dims.remove(ds.tos.cf[\"T\"].name)\n",
    "        weighted_average = tos_nino34_anom.weighted(ds[\"areacello\"]).mean(dim=list(non_time_dims))\n",
    "\n",
    "        # Calculate the rolling average\n",
    "        rolling_average = weighted_average.rolling(time=5, center=True).mean()\n",
    "        std_dev = weighted_average.std()\n",
    "        return rolling_average / std_dev\n",
    "\n",
    "    def add_enso_thresholds(da, threshold=0.4):\n",
    "\n",
    "        # Conver the xr.DataArray into an xr.Dataset\n",
    "        ds = da.to_dataset()\n",
    "\n",
    "        # Cleanup the time and use the thresholds\n",
    "        try:\n",
    "            ds[\"time\"]= ds.indexes[\"time\"].to_datetimeindex()\n",
    "        except:\n",
    "            pass\n",
    "        ds[\"tos_gt_04\"] = (\"time\", ds.tos.where(ds.tos >= threshold, threshold).data)\n",
    "        ds[\"tos_lt_04\"] = (\"time\", ds.tos.where(ds.tos <= -threshold, -threshold).data)\n",
    "\n",
    "        # Add fields for the thresholds\n",
    "        ds[\"el_nino_threshold\"] = (\"time\", np.zeros_like(ds.tos) + threshold)\n",
    "        ds[\"la_nina_threshold\"] = (\"time\", np.zeros_like(ds.tos) - threshold)\n",
    "\n",
    "        return ds\n",
    "    \n",
    "    ds = search_esgf(\"NCAR\")\n",
    "    enso_index = add_enso_thresholds(calculate_enso(ds).compute())\n",
    "    enso_index.attrs = ds.attrs\n",
    "    enso_index.attrs[\"model\"] = model\n",
    "\n",
    "    return enso_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad93de-5473-4579-8ee4-cadd0fbb90b2",
   "metadata": {},
   "source": [
    "## Configure Globus Compute\n",
    "\n",
    "Now that we have our functions, we can move toward using [Globus Flows](https://www.globus.org/globus-flows-service) and [Globus Compute](https://www.globus.org/compute).\n",
    "\n",
    "Globus Flows is a reliable and secure platform for orchestrating and performing research data management and analysis tasks. A flow is often needed to manage data coming from instruments, e.g., image files can be moved from local storage attached to a microscope to a high-performance storage system where they may be accessed by all members of the research project.\n",
    "\n",
    "More examples of creating and running flows can be found on our [demo instance](https://jupyter.demo.globus.org/hub/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663dfed0-e099-43db-98ad-9eb5021ac69e",
   "metadata": {},
   "source": [
    "### Setup a Globus Compute Endpoint\n",
    "Globus Compute (GC) is a service that allows **python functions** to be sent to remote points, executed, with the output from that function returned to the user. While there are a collection of endpoints already installed, we highlight in this section the steps required to configure for yourself. This idea is also known as \"serverless\" computing, where users do not need to think about the underlying infrastructure executing the code, but rather submit functions to be run and returned.\n",
    "\n",
    "To start a GC endpoint at your system you need to login, [configure a conda environment](https://foundations.projectpythia.org/foundations/how-to-run-python.html#installing-and-managing-python-with-conda), and `pip install globus-compute-endpoint`.\n",
    "\n",
    "You can then run:\n",
    "\n",
    "```globus-compute-endpoint configure esgf-test```\n",
    "\n",
    "```globus-compute-endpoint start esgf-test```\n",
    "\n",
    "Note that by default your endpoint will execute tasks on the login node (if you are using a High Performance Compute System). Additional configuration is needed for the endpoint to provision compute nodes. For example, here is the documentation on configuring globus compute endpoints on the Argonne Leadership Computing Facility's Polaris system\n",
    "- https://globus-compute.readthedocs.io/en/latest/endpoints.html#polaris-alcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d9e8b-e38d-41a5-b5f6-df9916d69f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_id = \"b3d1d669-d49b-412e-af81-95f3368e525c\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef408588-1e81-4726-892b-a9b0ad2f38cc",
   "metadata": {},
   "source": [
    "### Setup an Executor to Run our Functions\n",
    "Once we have our compute endpoint ID, we need to pass this to our executor, which will be used to pass our functions from our local machine to the machine we would like to compute on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa43e9e-6840-4b46-9a0c-ceeef8ca7e1e",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "gce = Executor(endpoint_id=endpoint_id)\n",
    "gce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe4cec-fca9-40ed-b20a-39061ad1d45a",
   "metadata": {},
   "source": [
    "### Test our Functions\n",
    "Now that we have our functions prepared, and an executor to run on, we can test them out using our endpoint!\n",
    "\n",
    "We pass in our function name, and the additional arguments for our functions. For example, let's look at comparing at the NCAR and MIROC modeling center's CMIP6 simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c9fd2-8822-4e34-9c2b-8558c489e487",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "ncar_task = gce.submit(run_plot_enso, model='NCAR')\n",
    "miroc_task = gce.submit(run_plot_enso, model='MIROC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccffe7fe-f11c-4b43-9b1a-b140eb1aa8a5",
   "metadata": {},
   "source": [
    "The results are started as python objects, with the resultant datasets available using `.result()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f0f35-9847-43bb-8e4c-b42ba5060233",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "ncar_ds = ncar_task.result()\n",
    "miroc_ds = miroc_task.result()\n",
    "\n",
    "ncar_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1257d1a-9712-427b-b9ce-4db644420839",
   "metadata": {},
   "source": [
    "### Plot our Data\n",
    "Now that we have pre-computed datasets, the last step is to visualize the output. In the other example, we stepped through how to utilize the `.hvplot` tool to create interactive displays of ENSO values. We will utilize that functionality here, wrapping into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac34be7-4faa-417c-b607-d8ee094be3e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_enso(ds):\n",
    "    el_nino = ds.hvplot.area(x=\"time\", y2='tos_gt_04', y='el_nino_threshold', color='red', hover=False)\n",
    "    el_nino_label = hv.Text(ds.isel(time=40).time.values, 2, 'El Niño').opts(text_color='red',)\n",
    "\n",
    "    # Create the La Niña area graphs\n",
    "    la_nina = ds.hvplot.area(x=\"time\", y2='tos_lt_04', y='la_nina_threshold', color='blue', hover=False)\n",
    "    la_nina_label = hv.Text(ds.isel(time=-40).time.values, -2, 'La Niña').opts(text_color='blue')\n",
    "\n",
    "    # Plot a timeseries of the ENSO 3.4 index\n",
    "    enso = ds.tos.hvplot(x='time', line_width=0.5, color='k', xlabel='Year', ylabel='ENSO 3.4 Index')\n",
    "\n",
    "    # Combine all the plots into a single plot\n",
    "    return (el_nino_label * la_nina_label * el_nino * la_nina * enso).opts(title=f'{ds.attrs[\"model\"]} {ds.attrs[\"source_id\"]} \\n Ensemble Member: {ds.attrs[\"variant_label\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13492e2d-c32a-4bcc-b341-837d5ea91a1a",
   "metadata": {},
   "source": [
    "Once we have the function, we apply to our two datasets and combine into a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6332c80-0ee9-4a4f-a277-95efc2cc8252",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "(plot_enso(ncar_ds) + plot_enso(miroc_ds)).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bfceb9-124a-4d72-9d49-3ca255965e29",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, we applied the ENSO 3.4 index calculations to CMIP6 datasets remotely using Globus Compute and created interactive plots comparing where we see El Niño and La Niña.\n",
    "\n",
    "### What's next?\n",
    "We will see some more advanced examples of using the CMIP6 and other data access methods as well as computations.\n",
    "\n",
    "## Resources and references\n",
    "- [Intake-ESGF Documentation](https://github.com/nocollier/intake-esgf)\n",
    "- [Globus Compute Documentation](https://www.globus.org/compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2864f-6661-4aa4-8d65-8dc10c961b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
