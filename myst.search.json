{"version":"1","records":[{"hierarchy":{"lvl1":"CMIP6 Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"CMIP6 Cookbook"},"content":"\n\n\n\n\n\n\n\n\n\nThis Project Pythia Cookbook covers examples of analysis of Google Cloud CMIP6 data using Pangeo tools.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl2":"Motivation"},"content":"From the \n\nCMIP6 website:\n\nThe simulation data produced by models under previous phases of CMIP have been used in thousands of research papers ... and the multi-model results provide some perspective on errors and uncertainty in model simulations. This information has proved invaluable in preparing high profile reports assessing our understanding of climate and climate change (e.g., the IPCC Assessment Reports).\n\nWith such a large amount of model output produced, moving the data around is inefficient. In this collection of notebooks, you will learn how to access cloud-optimized CMIP6 datasets, in addition to a few examples of using that data to analyze some aspects of climate change.","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl2":"Authors"},"content":"Ryan Abernathey, \n\nHenri Drake, \n\nRobert Ford, \n\nMax Grover","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl2":"Structure"},"content":"","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl3":"Foundations","lvl2":"Structure"},"type":"lvl3","url":"/#foundations","position":10},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl3":"Foundations","lvl2":"Structure"},"content":"This section includes three variations of accessing CMIP6 data from cloud storage.","type":"content","url":"/#foundations","position":11},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl3":"Example workflows","lvl2":"Structure"},"type":"lvl3","url":"/#example-workflows","position":12},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl3":"Example workflows","lvl2":"Structure"},"content":"There are currently four examples of using this data to\n\nEstimate equilibrium climate sensitivity (ECS)\n\nPlot global mean surface temperature under two different \n\nShared Socioeconomic Pathways\n\nPlot changes in precipitation intensity under the SSP585 scenario\n\nCalculate changes in ocean heat uptake after regridding with xESMF","type":"content","url":"/#example-workflows","position":13},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":14},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":15},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":16},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.","type":"content","url":"/#running-on-binder","position":17},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":18},{"hierarchy":{"lvl1":"CMIP6 Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\nClone the https://github.com/ProjectPythia/cmip6-cookbook repository: git clone https://github.com/ProjectPythia/cmip6-cookbook.git\n\nMove into the cmip6-cookbook directorycd cmip6-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate cmip6-cookbook-dev\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab\n\nAt this point, you can interact with the notebooks! Make sure to check out the \n\n“Getting Started with Jupyter” content from the \n\nPythia Foundations material if you are new to Jupyter or need a refresher.","type":"content","url":"/#running-on-your-own-machine","position":19},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity"},"type":"lvl1","url":"/notebooks/example-workflows/ecs-cmip6","position":0},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity"},"content":"\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6","position":1},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#overview","position":2},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Overview"},"content":"Equilibrium Climate Sensitivity (ECS) is defined as change in global-mean near-surface air temperature (GMST) change due to an instantaneous doubling of CO2 concentrations and once the coupled ocean-atmosphere-sea ice system has acheived a statistical equilibrium (i.e. at the top-of-atmosphere, incoming solar shortwave radiation is balanced by reflected solar shortwave and outgoing thermal longwave radiation).\n\nThis notebook uses the “\n\nGregory method” to approximate the ECS of CMIP6 models based on the first 150 years after an abrupt doubling of CO2\nconcentrations. The Gregory method extrapolates the quasi-linear relationship between GMST and radiative imbalance at the top-of-atmosphere to estimate how much warming would occur if the system were in radiative balance at the top-of-atmosphere, which is by definition the equilibrium response. In particular, we extrapolate the linear relationship that occurs between 100 and 150 years after the abrupt quadrupling.\n\nSince the radiative forcing due to CO2 is a logarithmic function of the CO2 concentration, the GMST change from a first doubling is roughly the same as for a second doubling (to first order, we can assume feedbacks as constant), which means that the GMST change due to a quadrupling of CO2 is roughly \\Delta T_{4\\times\\mathrm{CO}_2}=2\\times\\mathrm{ECS}. See also \n\nMauritsen et al. 2019 for a detailed application of the Gregory method (with modifications) for the case of one specific CMIP6 model, the MPI-M Earth System Model.\n\nFor another take on applying the Gregory method to estimate ECS, see \n\nAngeline Pendergrass’ code.\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#overview","position":3},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#prerequisites","position":4},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Xarray\n\nNecessary\n\n\n\nUnderstanding of NetCDF\n\nHelpful\n\nFamiliarity with metadata structure\n\nDask\n\nHelpful\n\n\n\nClimate sensitivity\n\nHelpful\n\n\n\nTime to learn: 30 minutes\n\n\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#prerequisites","position":5},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#imports","position":6},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Imports"},"content":"\n\nfrom matplotlib import pyplot as plt\nimport sys\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport cartopy\nimport dask\nfrom tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\nimport intake\nimport fsspec\nfrom dask_gateway import Gateway\nfrom dask.distributed import Client\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = 12, 6\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#imports","position":7},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Compute Cluster"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#compute-cluster","position":8},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Compute Cluster"},"content":"\n\nHere we use a dask cluster to parallelize our analysis.\n\nplatform = sys.platform\n\nif (platform == 'win32'):\n    import multiprocessing.popen_spawn_win32\nelse:\n    import multiprocessing.popen_spawn_posix\n\nInitiate the Dask client:\n\nclient = Client()\nclient\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#compute-cluster","position":9},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl3":"Data catalogs","lvl2":"Compute Cluster"},"type":"lvl3","url":"/notebooks/example-workflows/ecs-cmip6#data-catalogs","position":10},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl3":"Data catalogs","lvl2":"Compute Cluster"},"content":"This notebook uses \n\nintake-esm to ingest and organize climate model output from the fresh-off-the-supercomputers Phase 6 of the Coupled Model Intercomparison Project (CMIP6).\n\nThe file https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv in Google Cloud Storage contains thousands of lines of metadata, each describing an individual climate model experiment’s simulated data.\n\nFor example, the first line in the .csv file contains the precipitation rate (variable_id = 'pr'), as a function of latitude, longitude, and time, in an individual climate model experiment with the BCC-ESM1 model (source_id = 'BCC-ESM1') developed by the Beijing Climate Center (institution_id = 'BCC'). The model is forced by the forcing experiment SSP370 (experiment_id = 'ssp370'), which stands for the Shared Socio-Economic Pathway 3 that results in a change in radiative forcing of \\Delta F=7.0 W m-2 from pre-industrial to 2100. This simulation was run as part of the AerChemMIP activity, which is a spin-off of the CMIP activity that focuses specifically on how aerosol chemistry affects climate.\n\ndf = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\ndf.head()\n\nThe file pangeo-cmip6.json describes the structure of the CMIP6 metadata and is formatted so as to be read in by the intake.open_esm_datastore method, which categorizes all of the data pointers into a tiered collection. For example, this collection contains the simulated data from 28691 individual experiments, representing 48 different models from 23 different scientific institutions. There are 190 different climate variables (e.g. sea surface temperature, sea ice concentration, atmospheric winds, dissolved organic carbon in the ocean, etc.) available for 29 different forcing experiments.\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#data-catalogs","position":11},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Use Intake-ESM"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#use-intake-esm","position":12},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Use Intake-ESM"},"content":"\n\nIntake-ESM is a new package designed to make working with these data archives a bit simpler.\n\ncol = intake.open_esm_datastore(\"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\")\ncol\n\nHere, we show the various forcing experiments that climate modellers ran in these simulations.\n\ndf['experiment_id'].unique()\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#use-intake-esm","position":13},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Loading Data"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#loading-data","position":14},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Loading Data"},"content":"\n\nIntake-ESM enables loading data directly into an xarray.DataArray, a metadata-aware extension of numpy arrays. Xarray objects leverage \n\nDask to only read data into memory as needed for any specific operation (i.e. lazy evaluation). Think of Xarray Datasets as ways of conveniently organizing large arrays of floating point numbers (e.g. climate model data) on an n-dimensional discrete grid, with important metadata such as units, variable, names, etc.\n\nNote that data on the cloud are in \n\nZarr format, an extension of the metadata-aware format \n\nNetCDF commonly used in the geosciences.\n\nIntake-ESM has rules for aggegating datasets; these rules are defined in the collection-specification file.\n\nHere, we choose the piControl experiment (in which CO2 concentrations are held fixed at a pre-industrial level of ~300 ppm) and abrupt-2xCO2 experiment (in which CO2 concentrations are instantaneously doubled from a pre-industrial control state). Since the radiative forcing of CO2 is roughly a logarithmic function of CO2 concentrations, the ECS is roughly independent of the initial CO2 concentration.\n\nWarningThe version of this notebook in the \n\nPangeo Gallery uses the abrupt-4xCO2 forcing experiment, but fewer abrupt-2xCO2 datasets are currently avaiable in Google Cloud Storage, which significantly reduces run time. If you want to run this notebook on your own computer with the abrupt-4xCO2 experiment instead, change co2_option in the cell below. You will also need to take half of ecs, as described in the Overview.\n\nco2_option = 'abrupt-2xCO2'\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#loading-data","position":15},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Prepare Data"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#prepare-data","position":16},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Prepare Data"},"content":"\n\nquery = dict(\n    experiment_id=[co2_option,'piControl'], # pick the `abrupt-2xCO2` and `piControl` forcing experiments\n    table_id='Amon',                            # choose to look at atmospheric variables (A) saved at monthly resolution (mon)\n    variable_id=['tas', 'rsut','rsdt','rlut'],  # choose to look at near-surface air temperature (tas) as our variable\n    member_id = 'r1i1p1f1',                     # arbitrarily pick one realization for each model (i.e. just one set of initial conditions)\n)\n\ncol_subset = col.search(require_all_on=[\"source_id\"], **query)\ncol_subset.df.groupby(\"source_id\")[\n    [\"experiment_id\", \"variable_id\", \"table_id\"]\n].nunique()\n\nThe following functions help us load and homogenize the data. We use some \n\ndask.delayed programming to open the datasets in parallel.\n\ndef drop_all_bounds(ds):\n    \"\"\"Drop coordinates like 'time_bounds' from datasets,\n    which can lead to issues when merging.\"\"\"\n    drop_vars = [vname for vname in ds.coords\n                 if (('_bounds') in vname ) or ('_bnds') in vname]\n    return ds.drop_vars(drop_vars)\n\ndef open_dsets(df):\n    \"\"\"Open datasets from cloud storage and return xarray dataset.\"\"\"\n    dsets = [xr.open_zarr(fsspec.get_mapper(ds_url), consolidated=True)\n             .pipe(drop_all_bounds)\n             for ds_url in df.zstore]\n    try:\n        ds = xr.merge(dsets, join='exact')\n        return ds\n    except ValueError:\n        return None\n\ndef open_delayed(df):\n    \"\"\"A dask.delayed wrapper around `open_dsets`.\n    Allows us to open many datasets in parallel.\"\"\"\n    return dask.delayed(open_dsets)(df)\n\nCreate a nested dictionary of models and experiments. It will be structured like this:{'CESM2':\n  {\n    'piControl': <xarray.Dataset>,\n    'abrupt-2xCO2': <xarray.Dataset>\n  },\n  ...\n}\n\nfrom collections import defaultdict\n\ndsets = defaultdict(dict)\nfor group, df in col_subset.df.groupby(by=['source_id', 'experiment_id']):\n    dsets[group[0]][group[1]] = open_delayed(df)\n\nOpen one of the datasets directly, just to show what it looks like:\n\n%time open_dsets(df)\n\nNow use dask to do this in parallel on all of the datasets:\n\ndsets_ = dask.compute(dict(dsets))[0]\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#prepare-data","position":17},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Reduce Data via Global Mean"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#reduce-data-via-global-mean","position":18},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Reduce Data via Global Mean"},"content":"\n\nWe don’t want to load all of the raw model data into memory right away. Instead, we want to reduce the data by taking the global mean. We need to remember to weight this global mean by a factor proportional to cos(lat).\n\ndef get_lat_name(ds):\n    \"\"\"Figure out what is the latitude coordinate for each dataset.\"\"\"\n    for lat_name in ['lat', 'latitude']:\n        if lat_name in ds.coords:\n            return lat_name\n    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n\ndef global_mean(ds):\n    \"\"\"Return global mean of a whole dataset.\"\"\"\n    lat = ds[get_lat_name(ds)]\n    weight = np.cos(np.deg2rad(lat))\n    weight /= weight.mean()\n    other_dims = set(ds.dims) - {'time'}\n    return (ds * weight).mean(other_dims)\n\nWe now apply this function, plus resampling to annual mean data, to all of the datasets. We also concatenate the experiments together into a single Dataset for each model. This is the most complex cell in the notebook. A lot is happening here.\n\nexpts = ['piControl', co2_option]\nexpt_da = xr.DataArray(expts, dims='experiment_id',\n                       coords={'experiment_id': expts})\n\ndsets_aligned = {}\n\nfor k, v in tqdm(dsets_.items()):\n    expt_dsets = v.values()\n    if any([d is None for d in expt_dsets]):\n        print(f\"Missing experiment for {k}\")\n        continue\n\n    for ds in expt_dsets:\n        ds.coords['year'] = ds.time.dt.year - ds.time.dt.year[0]\n\n    # workaround for\n    # https://github.com/pydata/xarray/issues/2237#issuecomment-620961663\n    dsets_ann_mean = [v[expt].pipe(global_mean)\n                             .swap_dims({'time': 'year'})\n                             .drop_vars('time')\n                             .coarsen(year=12).mean()\n                      for expt in expts]\n\n    # align everything with the 2xCO2 experiment\n    dsets_aligned[k] = xr.concat(dsets_ann_mean, join='right',\n                                 dim=expt_da)\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#reduce-data-via-global-mean","position":19},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Do the Computation"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#do-the-computation","position":20},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Do the Computation"},"content":"\n\nUp to this point, no computations have actually happened. Everything has been “lazy”. Now we trigger the computation to actual occur and load the global/annual mean data into memory.\n\ndsets_aligned_ = dask.compute(dsets_aligned)[0]\n\nNow we concatenate across models to produce one big dataset with all the required variables.\n\nsource_ids = list(dsets_aligned_.keys())\nsource_da = xr.DataArray(source_ids, dims='source_id',\n                         coords={'source_id': source_ids})\n\nbig_ds = xr.concat([ds.reset_coords(drop=True)\n                    for ds in dsets_aligned_.values()],\n                   dim=source_da)\nbig_ds\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#do-the-computation","position":21},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Calculated Derived Variables"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#calculated-derived-variables","position":22},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Calculated Derived Variables"},"content":"\n\nWe need to calculate the net radiative imbalance, plus the anomaly of the abrupt 2xCO2 run compared to the piControl run.\n\nbig_ds['imbalance'] = big_ds['rsdt'] - big_ds['rsut'] - big_ds['rlut']\n\nds_mean = big_ds[['tas', 'imbalance']].sel(experiment_id='piControl').mean(dim='year')\nds_anom = big_ds[['tas', 'imbalance']] - ds_mean\n\n# add some metadata\nds_anom.tas.attrs['long_name'] = 'Global Mean Surface Temp Anom'\nds_anom.tas.attrs['units'] = 'K'\nds_anom.imbalance.attrs['long_name'] = 'Global Mean Radiative Imbalance'\nds_anom.imbalance.attrs['units'] = 'W m$^{-2}$'\n\nds_anom\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#calculated-derived-variables","position":23},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Plot Timeseries"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#plot-timeseries","position":24},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Plot Timeseries"},"content":"\n\nHere we plot the global mean surface temperature for each model:\n\nds_anom.tas.plot.line(col='source_id', x='year', col_wrap=5)\n\nWe can see that the models cover different time intervals. Let’s limit the rest of our analysis to the first 150 years.\n\nfirst_150_years = slice(0, 149)\nds_anom.tas.sel(year=first_150_years).plot.line(col='source_id', x='year', col_wrap=5)\n\nSame thing for radiative imbalance:\n\nds_anom.imbalance.sel(year=first_150_years).plot.line(col='source_id', x='year', col_wrap=5)\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#plot-timeseries","position":25},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Calculate ECS"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#calculate-ecs","position":26},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Calculate ECS"},"content":"\n\nds_abrupt = ds_anom.sel(year=first_150_years, experiment_id=co2_option).reset_coords(drop=True)\n\ndef calc_ecs(tas, imb):\n    a, b = np.polyfit(tas, imb, 1)\n    ecs = -1.0 * (b/a) # Change -1.0 to -0.5 if using 4xCO2\n    return xr.DataArray(ecs)\n\nds_abrupt['ecs'] = xr.apply_ufunc(calc_ecs, ds_abrupt.tas, ds_abrupt.imbalance, vectorize=True, input_core_dims=[['year'], ['year']])\nds_abrupt.compute()\n\nAlso, make sure that we set a couple of the variables to be coordinates.\n\nds_abrupt = ds_abrupt.set_coords(['tas', 'imbalance'])\n\nfg = ds_abrupt.plot.scatter(x='tas', y='imbalance', col='source_id', col_wrap=4, add_colorbar=False)\n\ndef calc_and_plot_ecs(x, y, **kwargs):\n    x = x[~np.isnan(x)]\n    y = y[~np.isnan(y)]\n    a, b = np.polyfit(x, y, 1)\n    ecs = -1.0 * b/a\n    plt.autoscale(False)\n    plt.plot([0, 10], np.polyval([a, b], [0, 10]), 'k')\n    plt.text(2, 3, f'ECS = {ecs:3.2f}', fontdict={'weight': 'bold', 'size': 12})\n    plt.grid()\n\nfg.map(calc_and_plot_ecs, 'tas', 'imbalance')\n\nds_abrupt.ecs.plot.hist();\n\nds_abrupt.ecs.to_dataframe().sort_values('ecs').plot(kind='bar')\n\nWe’re at the end of the notebook, so let’s shutdown our Dask cluster.\n\nclient.shutdown()\n\n\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#calculate-ecs","position":27},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#summary","position":28},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Summary"},"content":"In this notebook, we estimated ECS for a subset of CMIP6 models using the Gregory method.","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#summary","position":29},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/example-workflows/ecs-cmip6#whats-next","position":30},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl3":"What’s next?","lvl2":"Summary"},"content":"We will plot global average surface air temperature for a historical run and two branching emissions scenarios.\n\n","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#whats-next","position":31},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/example-workflows/ecs-cmip6#resources-and-references","position":32},{"hierarchy":{"lvl1":"Estimating Equilibrium Climate Sensitivity","lvl2":"Resources and references"},"content":"Original notebook in the Pangeo Gallery by Henri Drake and \n\nRyan Abernathey","type":"content","url":"/notebooks/example-workflows/ecs-cmip6#resources-and-references","position":33},{"hierarchy":{"lvl1":"Global Mean Surface Temperature"},"type":"lvl1","url":"/notebooks/example-workflows/gmst","position":0},{"hierarchy":{"lvl1":"Global Mean Surface Temperature"},"content":"\n\n","type":"content","url":"/notebooks/example-workflows/gmst","position":1},{"hierarchy":{"lvl1":"Global Mean Surface Temperature","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/example-workflows/gmst#overview","position":2},{"hierarchy":{"lvl1":"Global Mean Surface Temperature","lvl2":"Overview"},"content":"This notebook uses similar techniques to the \n\nECS notebook. Please refer to that notebook for details.\n\n","type":"content","url":"/notebooks/example-workflows/gmst#overview","position":3},{"hierarchy":{"lvl1":"Global Mean Surface Temperature","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/example-workflows/gmst#prerequisites","position":4},{"hierarchy":{"lvl1":"Global Mean Surface Temperature","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nUnderstanding of NetCDF\n\nHelpful\n\nFamiliarity with metadata structure\n\nSeaborn\n\nHelpful\n\n\n\nTime to learn: 10 minutes\n\n\n\n","type":"content","url":"/notebooks/example-workflows/gmst#prerequisites","position":5},{"hierarchy":{"lvl1":"Global Mean Surface Temperature","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/example-workflows/gmst#imports","position":6},{"hierarchy":{"lvl1":"Global Mean Surface Temperature","lvl2":"Imports"},"content":"\n\nfrom matplotlib import pyplot as plt\nimport xarray as xr\nimport numpy as np\nimport dask\nfrom dask.diagnostics import progress\nfrom tqdm.autonotebook import tqdm\nimport intake\nimport fsspec\nimport seaborn as sns\n\n%matplotlib inline\n\ncol = intake.open_esm_datastore(\"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\")\ncol\n\n[eid for eid in col.df['experiment_id'].unique() if 'ssp' in eid]\n\nThere is currently a significant amount of data for these runs:\n\nexpts = ['historical', 'ssp245', 'ssp585']\n\nquery = dict(\n    experiment_id=expts,\n    table_id='Amon',\n    variable_id=['tas'],\n    member_id = 'r1i1p1f1',\n)\n\ncol_subset = col.search(require_all_on=[\"source_id\"], **query)\ncol_subset.df.groupby(\"source_id\")[\n    [\"experiment_id\", \"variable_id\", \"table_id\"]\n].nunique()\n\ndef drop_all_bounds(ds):\n    drop_vars = [vname for vname in ds.coords\n                 if (('_bounds') in vname ) or ('_bnds') in vname]\n    return ds.drop(drop_vars)\n\ndef open_dset(df):\n    assert len(df) == 1\n    ds = xr.open_zarr(fsspec.get_mapper(df.zstore.values[0]), consolidated=True)\n    return drop_all_bounds(ds)\n\ndef open_delayed(df):\n    return dask.delayed(open_dset)(df)\n\nfrom collections import defaultdict\ndsets = defaultdict(dict)\n\nfor group, df in col_subset.df.groupby(by=['source_id', 'experiment_id']):\n    dsets[group[0]][group[1]] = open_delayed(df)\n\ndsets_ = dask.compute(dict(dsets))[0]\n\nCalculate global means:\n\ndef get_lat_name(ds):\n    for lat_name in ['lat', 'latitude']:\n        if lat_name in ds.coords:\n            return lat_name\n    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n\ndef global_mean(ds):\n    lat = ds[get_lat_name(ds)]\n    weight = np.cos(np.deg2rad(lat))\n    weight /= weight.mean()\n    other_dims = set(ds.dims) - {'time'}\n    return (ds * weight).mean(other_dims)\n\nexpt_da = xr.DataArray(expts, dims='experiment_id', name='experiment_id',\n                       coords={'experiment_id': expts})\n\ndsets_aligned = {}\n\nfor k, v in tqdm(dsets_.items()):\n    expt_dsets = v.values()\n    if any([d is None for d in expt_dsets]):\n        print(f\"Missing experiment for {k}\")\n        continue\n\n    for ds in expt_dsets:\n        ds.coords['year'] = ds.time.dt.year\n\n    # workaround for\n    # https://github.com/pydata/xarray/issues/2237#issuecomment-620961663\n    dsets_ann_mean = [v[expt].pipe(global_mean)\n                             .swap_dims({'time': 'year'})\n                             .drop('time')\n                             .coarsen(year=12).mean()\n                      for expt in expts]\n\n    # align everything with the 4xCO2 experiment\n    dsets_aligned[k] = xr.concat(dsets_ann_mean, join='outer',\n                                 dim=expt_da)\n\ndsets_aligned_ = dask.compute(dsets_aligned)[0]\n\nsource_ids = list(dsets_aligned_.keys())\nsource_da = xr.DataArray(source_ids, dims='source_id', name='source_id',\n                         coords={'source_id': source_ids})\n\nbig_ds = xr.concat([ds.reset_coords(drop=True)\n                    for ds in dsets_aligned_.values()],\n                    dim=source_da)\n\nbig_ds\n\ndf_all = big_ds.sel(year=slice(1900, 2100)).to_dataframe().reset_index()\ndf_all.head()\n\nsns.relplot(data=df_all,\n            x=\"year\", y=\"tas\", hue='experiment_id',\n            kind=\"line\", ci=\"sd\", aspect=2);\n\n\n\n","type":"content","url":"/notebooks/example-workflows/gmst#imports","position":7},{"hierarchy":{"lvl1":"Global Mean Surface Temperature","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/example-workflows/gmst#summary","position":8},{"hierarchy":{"lvl1":"Global Mean Surface Temperature","lvl2":"Summary"},"content":"In this notebook, we accessed data for historical, SSP245, and SSP585 runs from a collection of CMIP6 models and plotted the multimodel-mean global average surface air temperature for each run.","type":"content","url":"/notebooks/example-workflows/gmst#summary","position":9},{"hierarchy":{"lvl1":"Global Mean Surface Temperature","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/example-workflows/gmst#whats-next","position":10},{"hierarchy":{"lvl1":"Global Mean Surface Temperature","lvl3":"What’s next?","lvl2":"Summary"},"content":"We will use CMIP6 data to analyze precipitation intensity under a warming climate.\n\n","type":"content","url":"/notebooks/example-workflows/gmst#whats-next","position":11},{"hierarchy":{"lvl1":"Global Mean Surface Temperature","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/example-workflows/gmst#resources-and-references","position":12},{"hierarchy":{"lvl1":"Global Mean Surface Temperature","lvl2":"Resources and references"},"content":"Original notebook in the Pangeo Gallery by Henri Drake and \n\nRyan Abernathey","type":"content","url":"/notebooks/example-workflows/gmst#resources-and-references","position":13},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis"},"type":"lvl1","url":"/notebooks/example-workflows/precip-freq","position":0},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis"},"content":"\n\n","type":"content","url":"/notebooks/example-workflows/precip-freq","position":1},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/example-workflows/precip-freq#overview","position":2},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Overview"},"content":"This notebook shows an advanced analysis case. The calculation was inspired by Angie Pendergrass’s work on precipitation statistics, as described in the following websites / papers:\n\nPendergrass & Deser (2017)\n\nhttps://​climatedataguide​.ucar​.edu​/climate​-data​/gpcp​-daily​-global​-precipitation​-climatology​-project\n\nWe use \n\nxhistogram to calculate the distribution of precipitation intensity and its changes in a warming climate.\n\n","type":"content","url":"/notebooks/example-workflows/precip-freq#overview","position":3},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/example-workflows/precip-freq#prerequisites","position":4},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Xarray\n\nNecessary\n\n\n\nUnderstanding of NetCDF\n\nHelpful\n\nFamiliarity with metadata structure\n\nDask\n\nHelpful\n\n\n\nTime to learn: 5 minutes\n\n\n\n","type":"content","url":"/notebooks/example-workflows/precip-freq#prerequisites","position":5},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/example-workflows/precip-freq#imports","position":6},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Imports"},"content":"\n\nimport os\nimport sys\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport fsspec\nfrom tqdm.autonotebook import tqdm\nfrom xhistogram.xarray import histogram\nfrom dask_gateway import Gateway\nfrom dask.distributed import Client\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = 12, 6\n\n","type":"content","url":"/notebooks/example-workflows/precip-freq#imports","position":7},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Compute Cluster"},"type":"lvl2","url":"/notebooks/example-workflows/precip-freq#compute-cluster","position":8},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Compute Cluster"},"content":"\n\nHere we use a dask cluster to parallelize our analysis.\n\nplatform = sys.platform\n\nif (platform == 'win32'):\n    import multiprocessing.popen_spawn_win32\nelse:\n    import multiprocessing.popen_spawn_posix\n\nInitiate the Dask client:\n\nclient = Client()\nclient\n\n","type":"content","url":"/notebooks/example-workflows/precip-freq#compute-cluster","position":9},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Load Data Catalog"},"type":"lvl2","url":"/notebooks/example-workflows/precip-freq#load-data-catalog","position":10},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Load Data Catalog"},"content":"\n\ndf = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\ndf.head()\n\ndf_3hr_pr = df[(df.table_id == '3hr') & (df.variable_id == 'pr')]\nlen(df_3hr_pr)\n\nrun_counts = df_3hr_pr.groupby(['source_id', 'experiment_id'])['zstore'].count()\nrun_counts\n\nsource_ids = []\nexperiment_ids = ['historical', 'ssp585']\nfor name, group in df_3hr_pr.groupby('source_id'):\n    if all([expt in group.experiment_id.values\n            for expt in experiment_ids]):\n        source_ids.append(name)\nsource_ids\n\n# Use only one model. Otherwise it takes too long to run on GitHub.\nsource_ids = ['BCC-CSM2-MR']\n\ndef load_pr_data(source_id, expt_id):\n    \"\"\"\n    Load 3hr precip data for given source and expt ids\n    \"\"\"\n    uri = df_3hr_pr[(df_3hr_pr.source_id == source_id) &\n                         (df_3hr_pr.experiment_id == expt_id)].zstore.values[0]\n\n    ds = xr.open_zarr(fsspec.get_mapper(uri), consolidated=True)\n    return ds\n\ndef precip_hist(ds, nbins=100, pr_log_min=-3, pr_log_max=2):\n    \"\"\"\n    Calculate precipitation histogram for a single model.\n    Lazy.\n    \"\"\"\n    assert ds.pr.units == 'kg m-2 s-1'\n\n    # mm/day\n    bins_mm_day = np.hstack([[0], np.logspace(pr_log_min, pr_log_max, nbins)])\n    bins_kg_m2s = bins_mm_day / (24*60*60)\n\n    pr_hist = histogram(ds.pr, bins=[bins_kg_m2s], dim=['lon']).mean(dim='time')\n\n    log_bin_spacing = np.diff(np.log(bins_kg_m2s[1:3])).item()\n    pr_hist_norm = 100 * pr_hist / ds.dims['lon'] / log_bin_spacing\n    pr_hist_norm.attrs.update({'long_name': 'zonal mean rain frequency',\n                               'units': '%/Δln(r)'})\n    return pr_hist_norm\n\ndef precip_hist_for_expts(dsets, experiment_ids):\n    \"\"\"\n    Calculate histogram for a suite of experiments.\n    Eager.\n    \"\"\"\n    # actual data loading and computations happen in this next line\n    pr_hists = [precip_hist(ds).load()\n            for ds in [ds_hist, ds_ssp]]\n    pr_hist = xr.concat(pr_hists, dim=xr.Variable('experiment_id', experiment_ids))\n    return pr_hist\n\nresults = {}\nfor source_id in tqdm(source_ids):\n    # get a 20 year period\n    ds_hist = load_pr_data(source_id, 'historical').sel(time=slice('1980', '2000'))\n    ds_ssp = load_pr_data(source_id, 'ssp585').sel(time=slice('2080', '2100'))\n    pr_hist = precip_hist_for_expts([ds_hist, ds_ssp], experiment_ids)\n    results[source_id] = pr_hist\n\ndef plot_precip_changes(pr_hist, vmax=5):\n    \"\"\"\n    Visualize the output\n    \"\"\"\n    pr_hist_diff = (pr_hist.sel(experiment_id='ssp585') -\n                    pr_hist.sel(experiment_id='historical'))\n    pr_hist.sel(experiment_id='historical')[:, 1:].plot.contour(xscale='log', colors='0.5', levels=21)\n    pr_hist_diff[:, 1:].plot.contourf(xscale='log', vmax=vmax, levels=21)\n\ntitle = 'Change in Zonal Mean Rain Frequency'\nfor source_id, pr_hist in results.items():\n    plt.figure()\n    plot_precip_changes(pr_hist)\n    plt.title(f'{title}: {source_id}')\n\nWe’re at the end of the notebook, so let’s shutdown our Dask cluster.\n\nclient.shutdown()\n\n\n\n","type":"content","url":"/notebooks/example-workflows/precip-freq#load-data-catalog","position":11},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/example-workflows/precip-freq#summary","position":12},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Summary"},"content":"In this notebook, we used CMIP6 data to compare precipitation intensity in the SSP585 scenario to historical runs.","type":"content","url":"/notebooks/example-workflows/precip-freq#summary","position":13},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/example-workflows/precip-freq#whats-next","position":14},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl3":"What’s next?","lvl2":"Summary"},"content":"More examples of using CMIP6 data.\n\n","type":"content","url":"/notebooks/example-workflows/precip-freq#whats-next","position":15},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/example-workflows/precip-freq#resources-and-references","position":16},{"hierarchy":{"lvl1":"Precipitation Frequency Analysis","lvl2":"Resources and references"},"content":"Original notebook in the Pangeo Gallery by Henri Drake and \n\nRyan Abernathey","type":"content","url":"/notebooks/example-workflows/precip-freq#resources-and-references","position":17},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean"},"type":"lvl1","url":"/notebooks/example-workflows/xesmf-ohu","position":0},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean"},"content":"\n\n","type":"content","url":"/notebooks/example-workflows/xesmf-ohu","position":1},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/example-workflows/xesmf-ohu#overview","position":2},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Overview"},"content":"The main goal of this workflow is to calculate the mean change in ocean heat uptake (OHU) associated with the transient climate response (TCR) for CMIP6. TCR is defined as the change in global mean surface temperature at the time of CO2 doubling in a climate model run with a 1% increase in CO2 per year. The amount and pattern of heat uptake into the oceans are important in determining the strength of radiative feedbacks and thus climate sensitivity. See \n\nXie (2020) for an overview.\n\nIn order to use as many models as possible, we will need to load the model output in its native grid, then regrid to a common grid (here 1°x1° lat-lon) using \n\nxESMF. From there, we can take the average across models and either plot the result or save it as a netCDF file for later use.\n\n","type":"content","url":"/notebooks/example-workflows/xesmf-ohu#overview","position":3},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/example-workflows/xesmf-ohu#prerequisites","position":4},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Xarray\n\nNecessary\n\n\n\nComputations and Masks with Xarray\n\nNecessary\n\n\n\nLoad CMIP6 Data with Intake-ESM\n\nNecessary\n\n\n\nIntro to Cartopy\n\nHelpful\n\n\n\nUnderstanding of NetCDF\n\nHelpful\n\n\n\nFamiliarity with CMIP6\n\nHelpful\n\n\n\nTime to learn: 30 minutes\n\n\n\n","type":"content","url":"/notebooks/example-workflows/xesmf-ohu#prerequisites","position":5},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/example-workflows/xesmf-ohu#imports","position":6},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Imports"},"content":"\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport intake\nimport xesmf as xe\nfrom cartopy import crs as ccrs\nfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n\n","type":"content","url":"/notebooks/example-workflows/xesmf-ohu#imports","position":7},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Access the data"},"type":"lvl2","url":"/notebooks/example-workflows/xesmf-ohu#access-the-data","position":8},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Access the data"},"content":"\n\nFirst, we will open and search the Pangeo CMIP6 catalog for monthly hfds (downward heat flux at the sea surface) for the control (piControl) and 1%/year CO2 (1pctCO2) runs for all available models on their native grids. The argument require_all_on='source_id' ensures that each model used has both experiments required for this analysis.\n\ncat_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\ncol = intake.open_esm_datastore(cat_url)\n\nquery = dict(experiment_id=['1pctCO2', 'piControl'], table_id='Omon', variable_id='hfds', \n             grid_label='gn', member_id='r1i1p1f1', require_all_on='source_id')\n\ncat = col.search(**query)\ncat.df\n\nConveniently, NCAR contributed some data to CMIP6 that has already been regridded to a 1x1 lat-lon grid, which is the resolution I am interested in for the ensemble mean. We will use the coordinates from this Dataset when we create the xESMF regridder.\n\nrg_query = dict(source_id='CESM2', experiment_id='piControl', table_id='Omon', variable_id='hfds', \n             grid_label='gr', member_id='r1i1p1f1', require_all_on=['source_id'])\n\nrg_cat = col.search(**rg_query)\nrg_cat.df\n\nNow, make the dictionaries with the data:\n\ndset_dict = cat.to_dataset_dict(zarr_kwargs={'use_cftime':True})\nlist(dset_dict.keys())\n\nrg_dset_dict = rg_cat.to_dataset_dict(zarr_kwargs={'use_cftime':True})\nlist(rg_dset_dict.keys())\n\n","type":"content","url":"/notebooks/example-workflows/xesmf-ohu#access-the-data","position":9},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Define some functions and organize"},"type":"lvl2","url":"/notebooks/example-workflows/xesmf-ohu#define-some-functions-and-organize","position":10},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Define some functions and organize"},"content":"\n\nFirst, let’s make a function to get the diagnostic of interest: the change in ocean heat uptake at the time of transient CO2 doubling compared to the pre-industrial control:\n\ndef get_tcr(ctrl_key, expr_key):\n    ds_1pct = dset_dict[expr_key].squeeze()\n    ds_piCl = dset_dict[ctrl_key].squeeze()\n    ds_tcr = ds_1pct.isel(time=slice(12*59, 12*80)).mean(dim='time') - ds_piCl.isel(time=slice(12*59, 12*80)).mean(dim='time')\n    return ds_tcr\n\nNote that the time slice is 20 years centered around year 70, which is when CO2 doubles in a 1pctCO2 experiment (1.01^{70}\\approx 2). Just for convenience, we will also define a function that creates the xESMF regridder and performs the regridding. The regridder is specific to the input (ds_in) and output (regrid_to) grids, so it must be redefined for each model.\n\ndef regrid(ds_in, regrid_to, method='bilinear'):\n    regridder = xe.Regridder(ds_in, regrid_to, method=method, periodic=True, ignore_degenerate=True)\n    ds_out = regridder(ds_in)\n    return ds_out\n\nFinally, the following function takes the list of keys generated by Intake-ESM and splits them into two sorted lists of keys: one for the piControl experiment and another for 1pctCO2. This will work nicely with the get_tcr() function.\n\ndef sorted_split_list(a_list):\n    c_list = []\n    e_list = []\n    for item in a_list:\n        if 'piControl' in item:\n            c_list.append(item)\n        elif '1pctCO2' in item:\n            e_list.append(item)\n        else: print('Could not find experiment name in key:'+item)\n    return sorted(c_list), sorted(e_list)\n\nLet’s make the lists and look at them to make sure they are properly sorted:\n\nctrl_keys, expr_keys = sorted_split_list(list(dset_dict.keys()))\n\nfor i in range(len(ctrl_keys)):\n    print(ctrl_keys[i]+'\\t\\t'+expr_keys[i])\n\n","type":"content","url":"/notebooks/example-workflows/xesmf-ohu#define-some-functions-and-organize","position":11},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl3":"Note","lvl2":"Define some functions and organize"},"type":"lvl3","url":"/notebooks/example-workflows/xesmf-ohu#note","position":12},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl3":"Note","lvl2":"Define some functions and organize"},"content":"If you look at the hfds anomaly for SAM0-UNICON, you will see negative values around the North Atlantic, especially in the Labrador Sea and Denmark Strait. These are areas of deep water formation and ocean heat uptake. By the CMIP convention, as described in the hfds attributes, a negative hfds indicates an upward heat flux from the ocean to the atmosphere, so by physical reasoning, this data should have the opposite sign. We could do this manually, but for simplicity, let’s just remove the model from our analysis.\n\ndset_dict['CMIP.SNU.SAM0-UNICON.1pctCO2.Omon.gn']\n\nget_tcr('CMIP.SNU.SAM0-UNICON.piControl.Omon.gn', 'CMIP.SNU.SAM0-UNICON.1pctCO2.Omon.gn').hfds.plot()\n\nctrl_keys.pop(-3)\nexpr_keys.pop(-3)\n\nWe will also remove AWI-CM because it raises a MemoryError that causes this notebook to fail to \n\nexecute via binderbot. Feel free to add it back if this notebook is being run locally.\n\nctrl_keys.pop(1)\nexpr_keys.pop(1)\n\n","type":"content","url":"/notebooks/example-workflows/xesmf-ohu#note","position":13},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Regrid the data"},"type":"lvl2","url":"/notebooks/example-workflows/xesmf-ohu#regrid-the-data","position":14},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Regrid the data"},"content":"\n\nFirst, we will define the output grid. It does not matter what the data actually is, since we just want the structure of the Dataset.\n\nrg_ds = rg_dset_dict['CMIP.NCAR.CESM2.piControl.Omon.gr'].isel(time=0).squeeze()\nrg_ds\n\nHere we create a new dictionary to store our regridded data. The for-loop goes through the two sorted lists of keys and tries to regrid each model. This allows us to avoid removing a model and rerunning the code every time there is an error.\n\nTo summarize,\n\nGet the diagnostic of interest and try to regrid to a 1x1 lat-lon grid\n\nIf that fails for any reason, print the error\n\nIf the regridding is successful, add it to the new dictionary\n\nRepeat for all models\n\nds_regrid_dict = dict()\nsuccess_count = 0\nmodel_count = 0\n\nfor ctrl_key, expr_key in zip(ctrl_keys, expr_keys):\n    model = ctrl_key.split('.')[2]\n    try:\n        ds_tcr = get_tcr(ctrl_key=ctrl_key, expr_key=expr_key)\n        ds_tcr_hfds_regridded = regrid(ds_tcr, rg_ds, method='nearest_s2d').hfds\n    except Exception as e:\n        print('Failed to regrid '+model+': '+str(e))\n    else: \n        ds_regrid_dict[model] = ds_tcr_hfds_regridded\n        print(model+' regridded and added to dictionary')\n        success_count += 1\n    finally:\n        model_count += 1\n        \nprint('-'*40+'\\n| '+str(success_count)+'/'+str(model_count)+' models successfully regridded! |\\n'+'-'*40)\n\nCESM2-FV2 fails because of some issue with the dimensions of the coordinates. If we remove ignore_degenerate=True from the regridder defined in regrid(), there may be a few more failures because of a degenerate element: a cell that has corners close enough that the cell collapses to a line or point.\n\nNow we concat the results into a single DataArray:\n\nds = list(ds_regrid_dict.values())\ncoord = list(ds_regrid_dict.keys())\nds_out_regrid = xr.concat(objs=ds, dim=coord, coords='all').rename({'concat_dim':'model'})\nds_out_regrid\n\n","type":"content","url":"/notebooks/example-workflows/xesmf-ohu#regrid-the-data","position":15},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Plot or save the data"},"type":"lvl2","url":"/notebooks/example-workflows/xesmf-ohu#plot-or-save-the-data","position":16},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Plot or save the data"},"content":"\n\nThe following function extends lon by one grid point, giving it the value of the first point. This fixes a bug/feature of Cartopy where a vertical white line will appear at the “seam” of the plot. For example, if you have a dataset with longitudes [-179.5, 179.5] and make a plot centered on the Pacific, there will likely be a white line at 180. This is only for improving the look of the plot, so if you are doing further analysis or exporting to netCDF, skip this.\n\ndef add_cyclic_point(xarray_obj, dim, period=None):\n    if period is None:\n        period = xarray_obj.sizes[dim] / xarray_obj.coords[dim][:2].diff(dim).item()\n    first_point = xarray_obj.isel({dim: slice(1)})\n    first_point.coords[dim] = first_point.coords[dim]+period\n    return xr.concat([xarray_obj, first_point], dim=dim)\n\nNow we can take the ensemble mean and plot. Thanks to the work leading up to this point, it’s as simple as using Xarray’s .mean().\n\ncmip6em_ohutcr = add_cyclic_point(ds_out_regrid.mean(dim='model'), 'lon', period=360)\n# cmip6em_ohutcr.to_netcdf('cmip6_ohutcr.nc') # remove add_cyclic_point() and uncomment to save\ncmip6em_ohutcr\n\nfig = plt.figure(1, figsize=(12, 5), dpi=130)\nax_mean = plt.subplot(projection=ccrs.PlateCarree(central_longitude=-150))\nmean_plot = ax_mean.contourf(cmip6em_ohutcr.lon, cmip6em_ohutcr.lat, cmip6em_ohutcr, transform=ccrs.PlateCarree(), \n                             cmap='RdBu_r', levels=np.linspace(-35, 35, 15), extend='both')\nax_mean.set_title('CMIP6 ensemble-mean $\\Delta\\mathrm{OHUTCR}$')\nax_mean.coastlines()\nax_mean.set_xticks([-120, -60, 0, 60, 120, 180], crs=ccrs.PlateCarree())\nax_mean.set_yticks([-90, -60, -30, 0, 30, 60, 90], crs=ccrs.PlateCarree())\nax_mean.xaxis.set_major_formatter(LongitudeFormatter(zero_direction_label=True))\nax_mean.yaxis.set_major_formatter(LatitudeFormatter())\nplt.colorbar(mean_plot, orientation='vertical', label='W m$^{-2}$')\n\nNotice how the heat uptake is highest in the subpolar oceans, especially the North Atlantic. From this multi-model ensemble mean, we can see that this is a robust feature of climate models (and likely the climate system itself) in response to a CO2 forcing. For more background and motivation, see \n\nHu et al. (2020).\n\n\n\n","type":"content","url":"/notebooks/example-workflows/xesmf-ohu#plot-or-save-the-data","position":17},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/example-workflows/xesmf-ohu#summary","position":18},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Summary"},"content":"This notebook demonstrates the use of xESMF to regrid the CMIP6 data hosted in Pangeo’s Google cloud storage. The regridded data allows us to use Xarray to take a multi-model mean, in this case, of changes in ocean heat uptake associated with each model’s transient climate response.","type":"content","url":"/notebooks/example-workflows/xesmf-ohu#summary","position":19},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/example-workflows/xesmf-ohu#whats-next","position":20},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl3":"What’s next?","lvl2":"Summary"},"content":"Other example workflows using this CMIP6 cloud data.\n\n","type":"content","url":"/notebooks/example-workflows/xesmf-ohu#whats-next","position":21},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/example-workflows/xesmf-ohu#resources-and-references","position":22},{"hierarchy":{"lvl1":"Regridding with xESMF and calculating a multi-model mean","lvl2":"Resources and references"},"content":"Hu, S., Xie, S.-P., & Liu, W. (2020). Global Pattern Formation of Net Ocean Surface Heat Flux Response to Greenhouse Warming. Journal of Climate, 33(17), 7503–7522. \n\nHu et al. (2020)\n\nXie, S.-P. (2020). Ocean warming pattern effect on global and regional climate change. AGU Advances, 1, e2019AV000130. \n\nXie (2020)\n\nParts of this workflow were taken from a similar workflow in \n\nthis notebook by NordicESMhub.","type":"content","url":"/notebooks/example-workflows/xesmf-ohu#resources-and-references","position":23},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP"},"type":"lvl1","url":"/notebooks/foundations/esgf-opendap","position":0},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP"},"content":"\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap","position":1},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/foundations/esgf-opendap#overview","position":2},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Overview"},"content":"This notebook shows how to search and load data via \n\nEarth System Grid Federation infrastructure. This infrastructure works great and is the foundation of the CMIP6 distribution system.\n\nThe main technologies used here are the \n\nESGF search API, used to figure out what data we want, and \n\nOPeNDAP, a remote data access protocol over HTTP.\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap#overview","position":3},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/foundations/esgf-opendap#prerequisites","position":4},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Xarray\n\nNecessary\n\n\n\nUnderstanding of NetCDF\n\nHelpful\n\nFamiliarity with metadata structure\n\nTime to learn: 10 minutes\n\n\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap#prerequisites","position":5},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/foundations/esgf-opendap#imports","position":6},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Imports"},"content":"\n\nimport warnings\n\nfrom distributed import Client\nimport holoviews as hv\nimport hvplot.xarray\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pyesgf.search import SearchConnection\nimport xarray as xr\n\nxr.set_options(display_style='html')\nwarnings.filterwarnings(\"ignore\")\nhv.extension('bokeh')\n\nclient = Client()\nclient\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap#imports","position":7},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Search using ESGF API"},"type":"lvl2","url":"/notebooks/foundations/esgf-opendap#search-using-esgf-api","position":8},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Search using ESGF API"},"content":"Fortunately, there is an ESGF API implemented in Python - pyesgf, which requires three major steps:\n\nEstablish a search connection\n\nQuery your data\n\nExtract the urls to your data\n\nOnce you have this information, you can load the data into an xarray.Dataset.\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap#search-using-esgf-api","position":9},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Configure the connection to a data server","lvl2":"Search using ESGF API"},"type":"lvl3","url":"/notebooks/foundations/esgf-opendap#configure-the-connection-to-a-data-server","position":10},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Configure the connection to a data server","lvl2":"Search using ESGF API"},"content":"First, we configure our connection to some server, using the distributed option (distrib=False). In this case, we are searching from the Lawerence Livermore National Lab (LLNL) data node.\n\nconn = SearchConnection('https://esgf-node.llnl.gov/esg-search',\n                        distrib=False)\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap#configure-the-connection-to-a-data-server","position":11},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Query our dataset","lvl2":"Search using ESGF API"},"type":"lvl3","url":"/notebooks/foundations/esgf-opendap#query-our-dataset","position":12},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Query our dataset","lvl2":"Search using ESGF API"},"content":"We are interested in a single experiment from CMIP6 - one of the Community Earth System Model version 2 (CESM2) runs, specifically the historical part of the simulation.\n\nWe are also interested in a single variable - temperature at the surface (tas), with a single ensemble member (r10i1p1f1)\n\nctx = conn.new_context(\n    facets='project,experiment_id',\n    project='CMIP6',\n    table_id='Amon',\n    institution_id=\"NCAR\",\n    experiment_id='historical',\n    source_id='CESM2',\n    variable='tas',\n    variant_label='r10i1p1f1',\n)\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap#query-our-dataset","position":13},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Extract the OpenDAP urls","lvl2":"Search using ESGF API"},"type":"lvl3","url":"/notebooks/foundations/esgf-opendap#extract-the-opendap-urls","position":14},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Extract the OpenDAP urls","lvl2":"Search using ESGF API"},"content":"In order to access the datasets, we need the urls to the data. Once we have these, we can read the data remotely!\n\nresult = ctx.search()[0]\nfiles = result.file_context().search()\nfiles\n\nThe files object is not immediately helpful - we need to extract the opendap_url method from this.\n\nfiles[0].opendap_url\n\nWe can use this for the whole list using list comprehension, as shown below.\n\nopendap_urls = [file.opendap_url for file in files]\nopendap_urls\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap#extract-the-opendap-urls","position":15},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Read the data into an xarray.Dataset"},"type":"lvl2","url":"/notebooks/foundations/esgf-opendap#read-the-data-into-an-xarray-dataset","position":16},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Read the data into an xarray.Dataset"},"content":"Now that we have our urls to the data, we can use open multifile dataset (open_mfdataset) to read the data, combining the coordinates and chunking by time.\n\nXarray, together with the netCDF4 Python library, allow lazy loading.\n\nds = xr.open_mfdataset(opendap_urls,\n                       combine='by_coords',\n                       chunks={'time':480})\nds\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap#read-the-data-into-an-xarray-dataset","position":17},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Plot a quick look of the data","lvl2":"Read the data into an xarray.Dataset"},"type":"lvl3","url":"/notebooks/foundations/esgf-opendap#plot-a-quick-look-of-the-data","position":18},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Plot a quick look of the data","lvl2":"Read the data into an xarray.Dataset"},"content":"Now that we have the dataset, let’s plot a few quick looks of the data.\n\nds.tas.sel(time='1950-01').squeeze().plot(cmap='Spectral_r');\n\nThese are OPeNDAP endpoints. Xarray, together with the netCDF4 Python library, allow lazy loading.\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap#plot-a-quick-look-of-the-data","position":19},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Compute an area-weighted global average"},"type":"lvl2","url":"/notebooks/foundations/esgf-opendap#compute-an-area-weighted-global-average","position":20},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Compute an area-weighted global average"},"content":"Let’s apply some computation to this dataset. We would like to calculate the global average temperature. This requires weighting each of the grid cells properly, using the area.","type":"content","url":"/notebooks/foundations/esgf-opendap#compute-an-area-weighted-global-average","position":21},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Find the area of the cells","lvl2":"Compute an area-weighted global average"},"type":"lvl3","url":"/notebooks/foundations/esgf-opendap#find-the-area-of-the-cells","position":22},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Find the area of the cells","lvl2":"Compute an area-weighted global average"},"content":"We can query the dataserver again, this time extracting the area of the cell (areacella).\n\nctx = conn.new_context(\n    facets='project,experiment_id',\n    project='CMIP6',\n    institution_id=\"NCAR\",\n    experiment_id='historical',\n    source_id='CESM2',\n    variable='areacella',\n)\n\nAs before, we extract the opendap urls.\n\nresult = ctx.search()[0]\nfiles = result.file_context().search()\nopendap_urls = [file.opendap_url for file in files]\nopendap_urls\n\nAnd finally, we load our cell area file into an xarray.Dataset\n\nds_area = xr.open_dataset(opendap_urls[0])\nds_area\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap#find-the-area-of-the-cells","position":23},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Compute the global average","lvl2":"Compute an area-weighted global average"},"type":"lvl3","url":"/notebooks/foundations/esgf-opendap#compute-the-global-average","position":24},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Compute the global average","lvl2":"Compute an area-weighted global average"},"content":"Now that we have the area of each cell, and the temperature at each point, we can compute the global average temperature.\n\ntotal_area = ds_area.areacella.sum(dim=['lon', 'lat'])\nta_timeseries = (ds.tas * ds_area.areacella).sum(dim=['lon', 'lat']) / total_area\nta_timeseries\n\nBy default the data are loaded lazily, as Dask arrays. Here we trigger computation explicitly.\n\n%time ta_timeseries.load()\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap#compute-the-global-average","position":25},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Visualize our results","lvl2":"Compute an area-weighted global average"},"type":"lvl3","url":"/notebooks/foundations/esgf-opendap#visualize-our-results","position":26},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"Visualize our results","lvl2":"Compute an area-weighted global average"},"content":"Now that we have our results, we can visualize using static and dynamic plots. Let’s start with static plots using matplotlib, then dynamic plots using hvPlot.\n\nta_timeseries['time'] = ta_timeseries.indexes['time'].to_datetimeindex()\n\nfig = plt.figure(figsize=(12,8))\nta_timeseries.plot(label='monthly')\nta_timeseries.rolling(time=12).mean().plot(label='12 month rolling mean')\nplt.legend()\nplt.title('Global Mean Surface Air Temperature')\n\nta_timeseries.name = 'Temperature (K)'\nmonthly_average = ta_timeseries.hvplot(title = 'Global Mean Surface Air Temperature',\n                                       label='monthly')\nrolling_monthly_average = ta_timeseries.rolling(time=12).mean().hvplot(label='12 month rolling mean',)\n\n(monthly_average * rolling_monthly_average).opts(legend_position='top_left')\n\n\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap#visualize-our-results","position":27},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/foundations/esgf-opendap#summary","position":28},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Summary"},"content":"In this notebook, we searched for and opened a CESM2 dataset using the ESGF API and OPeNDAP. We then plotted global average surface air temperature.","type":"content","url":"/notebooks/foundations/esgf-opendap#summary","position":29},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/foundations/esgf-opendap#whats-next","position":30},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl3":"What’s next?","lvl2":"Summary"},"content":"We will see some more advanced examples of using the CMIP6 data.\n\n","type":"content","url":"/notebooks/foundations/esgf-opendap#whats-next","position":31},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/foundations/esgf-opendap#resources-and-references","position":32},{"hierarchy":{"lvl1":"Search and Load CMIP6 Data via ESGF/OPeNDAP","lvl2":"Resources and references"},"content":"Original notebook in the Pangeo Gallery by Henri Drake and \n\nRyan Abernathey","type":"content","url":"/notebooks/foundations/esgf-opendap#resources-and-references","position":33},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example"},"type":"lvl1","url":"/notebooks/foundations/google-cloud-basic","position":0},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example"},"content":"\n\n","type":"content","url":"/notebooks/foundations/google-cloud-basic","position":1},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/foundations/google-cloud-basic#overview","position":2},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Overview"},"content":"This notebooks shows how to query the Google Cloud CMIP6 catalog and load the data using Python.\n\n","type":"content","url":"/notebooks/foundations/google-cloud-basic#overview","position":3},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/foundations/google-cloud-basic#prerequisites","position":4},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Xarray\n\nNecessary\n\n\n\nUnderstanding of NetCDF\n\nHelpful\n\nFamiliarity with metadata structure\n\nTime to learn: 10 minutes\n\n\n\n","type":"content","url":"/notebooks/foundations/google-cloud-basic#prerequisites","position":5},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/foundations/google-cloud-basic#imports","position":6},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Imports"},"content":"\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport zarr\nimport fsspec\nimport nc_time_axis\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = 12, 6\n\n","type":"content","url":"/notebooks/foundations/google-cloud-basic#imports","position":7},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Browse Catalog"},"type":"lvl2","url":"/notebooks/foundations/google-cloud-basic#browse-catalog","position":8},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Browse Catalog"},"content":"\n\nThe data catatalog is stored as a CSV file. Here we read it with Pandas.\n\ndf = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\ndf.head()\n\nThe columns of the dataframe correspond to the \n\nCMI6 controlled vocabulary.\n\nHere we filter the data to find monthly surface air temperature for historical experiments.\n\ndf_ta = df.query(\"activity_id=='CMIP' & table_id == 'Amon' & variable_id == 'tas' & experiment_id == 'historical'\")\ndf_ta\n\nNow we do further filtering to find just the models from NCAR.\n\ndf_ta_ncar = df_ta.query('institution_id == \"NCAR\"')\ndf_ta_ncar\n\n","type":"content","url":"/notebooks/foundations/google-cloud-basic#browse-catalog","position":9},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Load Data"},"type":"lvl2","url":"/notebooks/foundations/google-cloud-basic#load-data","position":10},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Load Data"},"content":"\n\nNow we will load a single store using fsspec, zarr, and xarray.\n\n# get the path to a specific zarr store (the first one from the dataframe above)\nzstore = df_ta_ncar.zstore.values[-1]\nprint(zstore)\n\n# create a mutable-mapping-style interface to the store\nmapper = fsspec.get_mapper(zstore)\n\n# open it using xarray and zarr\nds = xr.open_zarr(mapper, consolidated=True)\nds\n\n","type":"content","url":"/notebooks/foundations/google-cloud-basic#load-data","position":11},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Plot the Data"},"type":"lvl2","url":"/notebooks/foundations/google-cloud-basic#plot-the-data","position":12},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Plot the Data"},"content":"Plot a map from a specific date:\n\nds.tas.sel(time='1950-01').squeeze().plot()\n\nThe global mean of a lat-lon field needs to be weighted by the area of each grid cell, which is proportional to the cosine of its latitude.\n\ndef global_mean(field):\n    weights = np.cos(np.deg2rad(field.lat))\n    return field.weighted(weights).mean(dim=['lat', 'lon'])\n\nWe can pass all of the temperature data through this function:\n\nta_timeseries = global_mean(ds.tas)\nta_timeseries\n\nBy default the data are loaded lazily, as Dask arrays. Here we trigger computation explicitly.\n\n%time ta_timeseries.load()\n\nta_timeseries.plot(label='monthly')\nta_timeseries.rolling(time=12).mean().plot(label='12 month rolling mean', color='k')\nplt.legend()\nplt.grid()\nplt.title('Global Mean Surface Air Temperature')\n\n\n\n","type":"content","url":"/notebooks/foundations/google-cloud-basic#plot-the-data","position":13},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/foundations/google-cloud-basic#summary","position":14},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Summary"},"content":"In this notebook, we opened a CESM2 dataset with fsspec and zarr. We calculated and plotted global average surface air temperature.","type":"content","url":"/notebooks/foundations/google-cloud-basic#summary","position":15},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/foundations/google-cloud-basic#whats-next","position":16},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl3":"What’s next?","lvl2":"Summary"},"content":"We will open a dataset with ESGF and OPenDAP.\n\n","type":"content","url":"/notebooks/foundations/google-cloud-basic#whats-next","position":17},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/foundations/google-cloud-basic#resources-and-references","position":18},{"hierarchy":{"lvl1":"Google Cloud CMIP6 Public Data: Basic Python Example","lvl2":"Resources and references"},"content":"Original notebook in the Pangeo Gallery by Henri Drake and \n\nRyan Abernathey","type":"content","url":"/notebooks/foundations/google-cloud-basic#resources-and-references","position":19},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM"},"type":"lvl1","url":"/notebooks/foundations/intake-esm","position":0},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM"},"content":"\n\n","type":"content","url":"/notebooks/foundations/intake-esm","position":1},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/foundations/intake-esm#overview","position":2},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl2":"Overview"},"content":"Intake-ESM is an experimental new package that aims to provide a higher-level interface to searching and loading Earth System Model data archives, such as CMIP6. The package is under very active development, and features may be unstable. Please report any \n\nissues or suggestions on GitHub.\n\n","type":"content","url":"/notebooks/foundations/intake-esm#overview","position":3},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/foundations/intake-esm#prerequisites","position":4},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Xarray\n\nNecessary\n\n\n\nUnderstanding of NetCDF\n\nHelpful\n\nFamiliarity with metadata structure\n\nTime to learn: 5 minutes\n\n\n\n","type":"content","url":"/notebooks/foundations/intake-esm#prerequisites","position":5},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/foundations/intake-esm#imports","position":6},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl2":"Imports"},"content":"\n\nimport xarray as xr\nxr.set_options(display_style='html')\nimport intake\n%matplotlib inline\n\n","type":"content","url":"/notebooks/foundations/intake-esm#imports","position":7},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl2":"Loading Data"},"type":"lvl2","url":"/notebooks/foundations/intake-esm#loading-data","position":8},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl2":"Loading Data"},"content":"\n\nIntake ESM works by parsing an \n\nESM Collection Spec and converting it to an \n\nIntake catalog. The collection spec is stored in a .json file. Here we open it using Intake.\n\ncat_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\ncol = intake.open_esm_datastore(cat_url)\ncol\n\nWe can now use Intake methods to search the collection, and, if desired, export a Pandas dataframe.\n\ncat = col.search(experiment_id=['historical', 'ssp585'], table_id='Oyr', variable_id='o2',\n                 grid_label='gn')\ncat.df\n\nIntake knows how to automatically open the Datasets using Xarray. Furthermore, Intake-ESM contains special logic to concatenate and merge the individual results of our query into larger, more high-level aggregated Xarray Datasets.\n\ndset_dict = cat.to_dataset_dict(zarr_kwargs={'consolidated': True})\nlist(dset_dict.keys())\n\nds = dset_dict['CMIP.CCCma.CanESM5.historical.Oyr.gn']\nds\n\n\n\n","type":"content","url":"/notebooks/foundations/intake-esm#loading-data","position":9},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/foundations/intake-esm#summary","position":10},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl2":"Summary"},"content":"In this notebook, we used Intake-ESM to open an Xarray Dataset for one particular model and experiment.","type":"content","url":"/notebooks/foundations/intake-esm#summary","position":11},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/foundations/intake-esm#whats-next","position":12},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl3":"What’s next?","lvl2":"Summary"},"content":"We will see an example of downloading a dataset with fsspec and zarr.\n\n","type":"content","url":"/notebooks/foundations/intake-esm#whats-next","position":13},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/foundations/intake-esm#resources-and-references","position":14},{"hierarchy":{"lvl1":"Load CMIP6 Data with Intake-ESM","lvl2":"Resources and references"},"content":"Original notebook in the Pangeo Gallery by Henri Drake and \n\nRyan Abernathey","type":"content","url":"/notebooks/foundations/intake-esm#resources-and-references","position":15},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in Project Pythia’s CMIP6 Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1}]}